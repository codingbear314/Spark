{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sysh83tiVYGZUC_4BKWX48c8F20A8i25",
      "authorship_tag": "ABX9TyPxo4NsMqhtQC5uP6lbuPPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingbear314/Spark/blob/main/FinanceModel001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data from yahoo finance"
      ],
      "metadata": {
        "id": "_NU-lded1alH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gsEfeGzLviSq"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "def Load_Dataset():\n",
        "  dataset = []\n",
        "\n",
        "  company_list = [\n",
        "                    'AAPL',\n",
        "                    'GOOG',\n",
        "                    'NVDA',\n",
        "                    'TSLA',\n",
        "                    '005930.KS',\n",
        "                    '000660.KS',\n",
        "                    '035420.KS',\n",
        "                    '035720.KS',\n",
        "                    'MSFT',\n",
        "                    'GOOGL',\n",
        "                    'AMZN',\n",
        "                    'META',\n",
        "                    'AMD',\n",
        "                    'V',\n",
        "                    'BRK-B',\n",
        "                    'JNJ',\n",
        "                    'BABA',\n",
        "                    'TSM',\n",
        "                    'PG'\n",
        "                  ]\n",
        "\n",
        "  for company in company_list:\n",
        "      slidingWindow = deque()\n",
        "      ticker = yf.Ticker(company)\n",
        "      data = ticker.history(interval = '1d', period = 'max', auto_adjust = True)\n",
        "\n",
        "      Open = list(data['Open'])\n",
        "      Close = list(data['Close'])\n",
        "      High = list(data['High'])\n",
        "      Low = list(data['Low'])\n",
        "\n",
        "      if len(Open) < 61:\n",
        "          print(f\"{company} Doesn't have enough data!\")\n",
        "          continue\n",
        "      addindex = 61\n",
        "      for i in range(addindex):\n",
        "          slidingWindow.append(torch.tensor([100000*Open[i], 100000*High[i], 100000*Low[i], 100000*Close[i]], dtype = torch.float32))\n",
        "      while addindex+1 < len(Open):\n",
        "          dataset.append(\n",
        "              (\n",
        "                  torch.stack(list(slidingWindow)[:-1]),\n",
        "                  list(slidingWindow)[-1]\n",
        "              )\n",
        "          )\n",
        "          addindex += 1\n",
        "          slidingWindow.append(torch.tensor([100000*Open[addindex], 100000*High[addindex], 100000*Low[addindex], 100000*Close[addindex]], dtype = torch.float32))\n",
        "          slidingWindow.popleft()\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "CAxupCtj1VDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFR9h8Lf1W0V",
        "outputId": "8643de11-dcef-4def-fe85-8fffe02fb0cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model structure\n",
        "\n",
        "* 60*4 input layer\n",
        "* Convolution layer (Conv1d, size of 8, input channel 4 to output channel 16)\n",
        "* Convolution layer (Conv1d, size of 8, input channel 16 to output channel 32)\n",
        "* Fully connected layer (Linear, size of 40 to 20)\n",
        "* Fully connected layer (Linear, size of 20 to 4)\n",
        "* 4 output layer\n",
        "\n",
        "All activations ReLU"
      ],
      "metadata": {
        "id": "lSYtgvvf5kro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Finance_001_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Finance_001_Model, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=8)\n",
        "        conv1_out_side = 60 - 8 + 1\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=8)\n",
        "        conv2_out_side = conv1_out_side - 8 + 1\n",
        "        self.dense1 = nn.Linear(32 * conv2_out_side, 40)\n",
        "        self.dense2 = nn.Linear(40, 20)\n",
        "        self.dense3 = nn.Linear(20, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.permute(0, 2, 1)\n",
        "      x = self.relu(self.conv1(x))\n",
        "      x = self.relu(self.conv2(x))\n",
        "      batch_size = x.size(0)\n",
        "      x = x.view(batch_size, -1)\n",
        "      x = self.relu(self.dense1(x))\n",
        "      x = self.relu(self.dense2(x))\n",
        "      x = self.dense3(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "1t5AAZ-O5ll7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "gri4PC7g5ql1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FinanceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[idx]"
      ],
      "metadata": {
        "id": "cffGOyYm5r49"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset"
      ],
      "metadata": {
        "id": "3HL2qS6o6XDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FinanceDataset = FinanceDataset(Load_Dataset())\n",
        "train_size = int(0.8 * len(FinanceDataset))\n",
        "validation_size = int(0.1 * len(FinanceDataset))\n",
        "test_size = len(FinanceDataset) - train_size - validation_size\n",
        "train_dataset, test_dataset, validation_dataset = torch.utils.data.random_split(FinanceDataset, [train_size, test_size, validation_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(validation_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "Train_Dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "Validation_Dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
        "Test_Dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKsGPWNA65hi",
        "outputId": "57ca69b2-58b4-4ae4-926f-94797affe881"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 108816\n",
            "Validation size: 13602\n",
            "Test size: 13603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Finance_001_Model().to(device)\n",
        "print(f\"Training on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xty6m26P7cBz",
        "outputId": "9c85094d-1af8-4564-dd5a-c346db12de2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "critertion = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "WdlOUEaO7jvV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "AN4uJdLJ4UWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Spark/model_21.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYYRh6kO4Wmw",
        "outputId": "001d5feb-2d4f-4e14-ba0c-ee348ffc1374"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "2qIj_6x27s0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, validation_dataloader, critertion, optimizer, epochs):\n",
        "    lowest_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "      model.train()\n",
        "      epoch_loss = 0.0\n",
        "      for X_batch, Y_batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch.to(device))\n",
        "        loss = critertion(output, Y_batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * X_batch.size(0)\n",
        "      train_loss = epoch_loss / len(train_dataloader.dataset)\n",
        "\n",
        "      model.eval()\n",
        "      validation_loss = 0.0\n",
        "      with torch.inference_mode():\n",
        "        for X_batch, Y_batch in validation_dataloader:\n",
        "          output = model(X_batch.to(device))\n",
        "          loss = critertion(output, Y_batch.to(device))\n",
        "          validation_loss += loss.item() * X_batch.size(0)\n",
        "      validation_loss = validation_loss / len(validation_dataloader.dataset)\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "      if validation_loss < lowest_val_loss:\n",
        "        lowest_val_loss = validation_loss\n",
        "        torch.save(model.state_dict(), f\"/content/drive/My Drive/Spark/best_model.pt\")\n",
        "      if (epoch+1) % 10 == 0:\n",
        "        torch.save(model.state_dict(), f\"/content/drive/My Drive/Spark/model_{epoch+1}.pt\")"
      ],
      "metadata": {
        "id": "ceDGkLU47uTx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, Train_Dataloader, Validation_Dataloader, critertion, optimizer, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTBAZDjlRv-P",
        "outputId": "5240146a-4d7d-412a-aa45-8d4ef3503665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 10857947780538500.0000, Validation Loss: 15062899117218174.0000\n",
            "Epoch 2/100, Train Loss: 10785193893884118.0000, Validation Loss: 6277810411159329.0000\n",
            "Epoch 3/100, Train Loss: 11208698329095952.0000, Validation Loss: 8392654671720947.0000\n",
            "Epoch 4/100, Train Loss: 10794569711622302.0000, Validation Loss: 5535583741290856.0000\n",
            "Epoch 5/100, Train Loss: 10403450123683722.0000, Validation Loss: 6165869712081561.0000\n",
            "Epoch 6/100, Train Loss: 13183485935090028.0000, Validation Loss: 6128452223791353.0000\n",
            "Epoch 7/100, Train Loss: 9802963069023158.0000, Validation Loss: 10628638929562594.0000\n",
            "Epoch 8/100, Train Loss: 10538949644571538.0000, Validation Loss: 14973881210106184.0000\n",
            "Epoch 9/100, Train Loss: 11556666845843356.0000, Validation Loss: 7953207129890321.0000\n",
            "Epoch 10/100, Train Loss: 9955421552362484.0000, Validation Loss: 5579894407315807.0000\n",
            "Epoch 11/100, Train Loss: 11781470549942482.0000, Validation Loss: 40162329170674896.0000\n",
            "Epoch 12/100, Train Loss: 10920432686741300.0000, Validation Loss: 20663035833351420.0000\n",
            "Epoch 13/100, Train Loss: 11119946874976148.0000, Validation Loss: 5729589437086047.0000\n",
            "Epoch 14/100, Train Loss: 10641874236733872.0000, Validation Loss: 12371308798050354.0000\n",
            "Epoch 15/100, Train Loss: 11878761379287316.0000, Validation Loss: 10168967893893278.0000\n",
            "Epoch 16/100, Train Loss: 11100167200741278.0000, Validation Loss: 7934284423285167.0000\n",
            "Epoch 17/100, Train Loss: 10345163048894452.0000, Validation Loss: 46098186623549128.0000\n",
            "Epoch 18/100, Train Loss: 10186494317937132.0000, Validation Loss: 6491607639717856.0000\n",
            "Epoch 19/100, Train Loss: 9824021649405940.0000, Validation Loss: 18472381351727360.0000\n",
            "Epoch 20/100, Train Loss: 10651695170691914.0000, Validation Loss: 6295079623733710.0000\n",
            "Epoch 21/100, Train Loss: 10399836817836480.0000, Validation Loss: 13427043611096450.0000\n",
            "Epoch 22/100, Train Loss: 10707646113804354.0000, Validation Loss: 10742236523856482.0000\n",
            "Epoch 23/100, Train Loss: 10672696845195090.0000, Validation Loss: 11988736519566860.0000\n",
            "Epoch 24/100, Train Loss: 11269409196015266.0000, Validation Loss: 6251880905588570.0000\n",
            "Epoch 25/100, Train Loss: 13801678193969356.0000, Validation Loss: 8875448719595589.0000\n",
            "Epoch 26/100, Train Loss: 16282890753067606.0000, Validation Loss: 6004392714319103.0000\n",
            "Epoch 27/100, Train Loss: 11147183906631418.0000, Validation Loss: 6545802556078259.0000\n",
            "Epoch 28/100, Train Loss: 9965221808946894.0000, Validation Loss: 7553422910510841.0000\n",
            "Epoch 29/100, Train Loss: 10575894415703956.0000, Validation Loss: 6353174649726953.0000\n",
            "Epoch 30/100, Train Loss: 10067587202846626.0000, Validation Loss: 6154906771927371.0000\n",
            "Epoch 31/100, Train Loss: 10345064121430856.0000, Validation Loss: 10470173847573222.0000\n",
            "Epoch 32/100, Train Loss: 13305717426974296.0000, Validation Loss: 5816446631962449.0000\n",
            "Epoch 33/100, Train Loss: 9708780801761110.0000, Validation Loss: 7978043748472065.0000\n",
            "Epoch 34/100, Train Loss: 11338963524810282.0000, Validation Loss: 11737771549013256.0000\n",
            "Epoch 35/100, Train Loss: 9778930956054444.0000, Validation Loss: 20011821184429980.0000\n",
            "Epoch 36/100, Train Loss: 10833747778561134.0000, Validation Loss: 25706656899641144.0000\n",
            "Epoch 37/100, Train Loss: 10218782551091940.0000, Validation Loss: 15361432427263276.0000\n",
            "Epoch 38/100, Train Loss: 10075845509734982.0000, Validation Loss: 10754154193758802.0000\n",
            "Epoch 39/100, Train Loss: 9846329647207838.0000, Validation Loss: 6266338005737214.0000\n",
            "Epoch 40/100, Train Loss: 11015133245230368.0000, Validation Loss: 27207034550804588.0000\n",
            "Epoch 41/100, Train Loss: 11439899298886808.0000, Validation Loss: 6246370401063499.0000\n",
            "Epoch 42/100, Train Loss: 15265856267606842.0000, Validation Loss: 18115656634783000.0000\n",
            "Epoch 43/100, Train Loss: 11224784408416008.0000, Validation Loss: 12409777311020962.0000\n",
            "Epoch 44/100, Train Loss: 16725019517095846.0000, Validation Loss: 11135702598799780.0000\n",
            "Epoch 45/100, Train Loss: 8600314915200200.0000, Validation Loss: 6664467544520733.0000\n",
            "Epoch 46/100, Train Loss: 9879974183956850.0000, Validation Loss: 5775849720041489.0000\n",
            "Epoch 47/100, Train Loss: 11207532862045514.0000, Validation Loss: 30805514561586280.0000\n",
            "Epoch 48/100, Train Loss: 11331643784992704.0000, Validation Loss: 38218102049937144.0000\n",
            "Epoch 49/100, Train Loss: 8939478834201104.0000, Validation Loss: 9456896861141228.0000\n",
            "Epoch 50/100, Train Loss: 10438868354454918.0000, Validation Loss: 7719584572366043.0000\n",
            "Epoch 51/100, Train Loss: 14353783257155876.0000, Validation Loss: 10407166567032992.0000\n",
            "Epoch 52/100, Train Loss: 13762434841599686.0000, Validation Loss: 9795275090957264.0000\n",
            "Epoch 53/100, Train Loss: 10558013555309106.0000, Validation Loss: 9692573037352040.0000\n",
            "Epoch 54/100, Train Loss: 9199639599878796.0000, Validation Loss: 7594498604013904.0000\n",
            "Epoch 55/100, Train Loss: 10707533828625014.0000, Validation Loss: 21205172964224252.0000\n",
            "Epoch 56/100, Train Loss: 9704059353190868.0000, Validation Loss: 6505854978496409.0000\n",
            "Epoch 57/100, Train Loss: 9784294536291338.0000, Validation Loss: 9005991548838670.0000\n",
            "Epoch 58/100, Train Loss: 9119045502201366.0000, Validation Loss: 14999708387637528.0000\n",
            "Epoch 59/100, Train Loss: 10592900825955420.0000, Validation Loss: 12255282739666934.0000\n",
            "Epoch 60/100, Train Loss: 10838190595614578.0000, Validation Loss: 9213592474595182.0000\n"
          ]
        }
      ]
    }
  ]
}